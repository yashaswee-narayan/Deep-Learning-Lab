{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***YASHASWEE NARAYAN -- 25/RCO/07***"
      ],
      "metadata": {
        "id": "iRsNZxgzaDAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Imports & Dataset Loading**"
      ],
      "metadata": {
        "id": "KssLwTjmLqdF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "msidMXjgLNqQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Load & Preprocess MNIST**"
      ],
      "metadata": {
        "id": "D8XHl-MULxzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize (0–255 → 0–1)\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Flatten 28x28 → 784\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W-53chZLpuZ",
        "outputId": "e4b14c29-cb35-4038-9e49-bf1e59619251"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Train shape: (60000, 784)\n",
            "Test shape: (10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. One-Hot Encoding (Labels)**"
      ],
      "metadata": {
        "id": "szHgK1psL9YU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(y, num_classes=10):\n",
        "    one_hot_labels = np.zeros((y.size, num_classes))\n",
        "    one_hot_labels[np.arange(y.size), y] = 1\n",
        "    return one_hot_labels\n",
        "\n",
        "y_train_oh = one_hot(y_train)\n",
        "y_test_oh = one_hot(y_test)\n"
      ],
      "metadata": {
        "id": "TulydU6fL5FF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Activation Functions**"
      ],
      "metadata": {
        "id": "Qqj_ULQVL_cQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "def relu_derivative(Z):\n",
        "    return Z > 0\n",
        "\n",
        "def softmax(Z):\n",
        "    expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "    return expZ / np.sum(expZ, axis=1, keepdims=True)\n"
      ],
      "metadata": {
        "id": "i8TkfyQfMZEZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Loss Function (Categorical Cross-Entropy)**"
      ],
      "metadata": {
        "id": "eXjbsdAlMbOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(y_true, y_pred):\n",
        "    epsilon = 1e-9\n",
        "    return -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=1))\n"
      ],
      "metadata": {
        "id": "X9bA-jgAMZ-B"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Initialize Network Parameters**"
      ],
      "metadata": {
        "id": "8bmFIs7iY40c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Architecture:**\n",
        "\n",
        "### **Input: 784**\n",
        "\n",
        "### **Hidden layer: 128**\n",
        "\n",
        "### **Output: 10**"
      ],
      "metadata": {
        "id": "SrnRJM_TY6gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "input_size = 784\n",
        "hidden_size = 128\n",
        "output_size = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "\n",
        "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "b2 = np.zeros((1, output_size))\n"
      ],
      "metadata": {
        "id": "S2l0tzF1Y1zN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Forward Propagation**"
      ],
      "metadata": {
        "id": "iq30hf_AZT2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(X):\n",
        "    Z1 = X @ W1 + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = A1 @ W2 + b2\n",
        "    A2 = softmax(Z2)\n",
        "    return Z1, A1, Z2, A2\n"
      ],
      "metadata": {
        "id": "NOnt_kdBZQ0U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Backpropagation**"
      ],
      "metadata": {
        "id": "Ou2IyfErZZ9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(X, y, Z1, A1, A2):\n",
        "    global W1, b1, W2, b2\n",
        "\n",
        "    m = X.shape[0]\n",
        "\n",
        "    dZ2 = A2 - y\n",
        "    dW2 = A1.T @ dZ2 / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "    dA1 = dZ2 @ W2.T\n",
        "    dZ1 = dA1 * relu_derivative(Z1)\n",
        "    dW1 = X.T @ dZ1 / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Update parameters\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n"
      ],
      "metadata": {
        "id": "i8tgl59oZYxn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. Training Loop**"
      ],
      "metadata": {
        "id": "UxLlRfUXZfjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, X_train.shape[0], batch_size):\n",
        "        X_batch = X_train[i:i+batch_size]\n",
        "        y_batch = y_train_oh[i:i+batch_size]\n",
        "\n",
        "        Z1, A1, Z2, A2 = forward(X_batch)\n",
        "        backward(X_batch, y_batch, Z1, A1, A2)\n",
        "\n",
        "    # Loss every epoch\n",
        "    _, _, _, train_pred = forward(X_train)\n",
        "    loss = cross_entropy(y_train_oh, train_pred)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfYW3yAfZbP0",
        "outputId": "1fe97e8b-6bc5-4558-db95-d54c512b3609"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 1.0138\n",
            "Epoch 2/20, Loss: 0.5271\n",
            "Epoch 3/20, Loss: 0.4185\n",
            "Epoch 4/20, Loss: 0.3715\n",
            "Epoch 5/20, Loss: 0.3439\n",
            "Epoch 6/20, Loss: 0.3244\n",
            "Epoch 7/20, Loss: 0.3091\n",
            "Epoch 8/20, Loss: 0.2962\n",
            "Epoch 9/20, Loss: 0.2849\n",
            "Epoch 10/20, Loss: 0.2746\n",
            "Epoch 11/20, Loss: 0.2650\n",
            "Epoch 12/20, Loss: 0.2560\n",
            "Epoch 13/20, Loss: 0.2475\n",
            "Epoch 14/20, Loss: 0.2396\n",
            "Epoch 15/20, Loss: 0.2321\n",
            "Epoch 16/20, Loss: 0.2250\n",
            "Epoch 17/20, Loss: 0.2182\n",
            "Epoch 18/20, Loss: 0.2118\n",
            "Epoch 19/20, Loss: 0.2056\n",
            "Epoch 20/20, Loss: 0.1998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. Model Evaluation (Accuracy)**"
      ],
      "metadata": {
        "id": "ykjlgXEjZ8JQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(X, y_true):\n",
        "    _, _, _, preds = forward(X)\n",
        "    predicted_labels = np.argmax(preds, axis=1)\n",
        "    return np.mean(predicted_labels == y_true)\n",
        "\n",
        "train_acc = accuracy(X_train, y_train)\n",
        "test_acc = accuracy(X_test, y_test)\n",
        "\n",
        "print(\"Training Accuracy:\", train_acc)\n",
        "print(\"Test Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZW7-ykKrZ4_K",
        "outputId": "a9a21b99-7b3c-46f9-fcf3-6aeeffb4789c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.9441333333333334\n",
            "Test Accuracy: 0.9429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "81IVzFSIaNkH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}